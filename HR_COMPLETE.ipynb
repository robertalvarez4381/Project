{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8207632a",
   "metadata": {},
   "source": [
    "# <center>----HR Retention Analysis---- </center>\n",
    "### Step 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750c3a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Model Selection & Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Configuration for cleaner charts\n",
    "sns.set(style=\"whitegrid\")\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40533a",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a9c931",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'HR_comma_sep.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Step 2: Data Loading and Cleaning ---\u001b[39;00m\n\u001b[32m      2\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33mHR_comma_sep.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully loaded \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Rename columns for clarity\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rober\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rober\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rober\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rober\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rober\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'HR_comma_sep.csv'"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Data Loading and Cleaning ---\n",
    "file_path = 'HR_comma_sep.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "print(f\"Successfully loaded '{file_path}'.\")\n",
    "    \n",
    "# Rename columns for clarity\n",
    "data = data.rename(columns={\n",
    "    'satisfaction_level': 'SatisfactionLevel',\n",
    "    'last_evaluation': 'LastEvaluation',\n",
    "    'number_project': 'NumberofProjects',\n",
    "    'average_montly_hours': 'AverageMonthlyHours',\n",
    "    'time_spend_company': 'TimeSpentAtCompany',\n",
    "    'Work_accident': 'WorkAccident',\n",
    "    'left': 'Left', \n",
    "    'promotion_last_5years': 'PromotionsInLast5Years',\n",
    "    'sales': 'Department',\n",
    "    'salary': 'Salary'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Data Structure ---\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf856d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Missing Values ---\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b951d",
   "metadata": {},
   "source": [
    "## Step 3: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a18323",
   "metadata": {},
   "source": [
    "### 3.1 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f950212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: EDA\n",
    "# 3.1 Correlation Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation_matrix = data.select_dtypes(include=['float64', 'int64']).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdBu', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of HR Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67171d02",
   "metadata": {},
   "source": [
    "**Largest Factor**  \n",
    "We can see that satisfaction level has the highest coorelation with one's decision to stay or leave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7885e573",
   "metadata": {},
   "source": [
    "### 3.2 Turnover By Department  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cedc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Turnover By Department: Volume vs Rate\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# --- Chart 1: The Volume (Sorted by Size) ---\n",
    "# 1. Calculate the order (Biggest departments first)\n",
    "volume_order = data['Department'].value_counts().index\n",
    "\n",
    "sns.countplot(\n",
    "    y='Department', \n",
    "    hue='Left', \n",
    "    data=data, \n",
    "    palette='viridis',\n",
    "    order=volume_order, \n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('1. Turnover Volume')\n",
    "axes[0].set_xlabel('Number of Employees')\n",
    "axes[0].legend(title='Status', labels=['Stayed', 'Left'])\n",
    "\n",
    "\n",
    "# --- Chart 2: The Rate (Sorted by Risk) ---\n",
    "# Calculate rates\n",
    "dept_counts = data.groupby(['Department', 'Left']).size().unstack()\n",
    "dept_counts['Turnover_Rate'] = (dept_counts[1] / (dept_counts[0] + dept_counts[1])) * 100\n",
    "dept_counts = dept_counts.sort_values(by='Turnover_Rate', ascending=False)\n",
    "\n",
    "sns.barplot(\n",
    "    x=dept_counts['Turnover_Rate'], \n",
    "    y=dept_counts.index,  \n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('2. Turnover Rate (Risk Percentage)')\n",
    "axes[1].set_xlabel('Turnover Rate (%)')\n",
    "axes[1].set_ylabel('') \n",
    "\n",
    "# Add average line\n",
    "avg_turnover = (data['Left'].mean() * 100)\n",
    "axes[1].axvline(avg_turnover, color='red', linestyle='--', label=f'Avg: {avg_turnover:.1f}%')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab6f4d",
   "metadata": {},
   "source": [
    "**Volume vs Rate Analysis**  \n",
    " - **Chart 1 (Volume):** Sales and Technical departments face the highest *absolute* turnover, driven by their large headcount. This represents the primary workload for the recruiting team.\n",
    " - **Chart 2 (Risk Rate):** When normalized for size, **HR** and **Accounting** emerge as the highest-risk departments (~29% and ~26% respectively). \n",
    "\n",
    " **Key Insight:** While Sales loses the most people, HR employees are the most likely to quit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f04a2",
   "metadata": {},
   "source": [
    "### 3.3 Salary Level Distribution and Turnover Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c078246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Salary Analysis: Volume vs. Rate\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Define the logical order for salary (Low -> High)\n",
    "salary_order = ['low', 'medium', 'high']\n",
    "\n",
    "# --- Chart 1: The Volume (Count) ---\n",
    "sns.countplot(\n",
    "    x='Salary', \n",
    "    hue='Left', \n",
    "    data=data, \n",
    "    order=salary_order, \n",
    "    palette='viridis',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('1. Salary Turnover Volume (Headcount)')\n",
    "axes[0].set_xlabel('Salary Level')\n",
    "axes[0].set_ylabel('Number of Employees')\n",
    "axes[0].legend(title='Status', labels=['Stayed', 'Left'])\n",
    "\n",
    "\n",
    "# --- Chart 2: The Rate (Percentage) ---\n",
    "# 1. Calculate rates\n",
    "salary_counts = data.groupby(['Salary', 'Left']).size().unstack()\n",
    "salary_counts['Turnover_Rate'] = (salary_counts[1] / (salary_counts[0] + salary_counts[1])) * 100\n",
    "\n",
    "# 2. Reindex to ensure the bars are in the correct Low -> High order\n",
    "salary_counts = salary_counts.reindex(salary_order)\n",
    "\n",
    "sns.barplot(\n",
    "    x=salary_counts.index, \n",
    "    y=salary_counts['Turnover_Rate'], \n",
    "    order=salary_order,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('2. Salary Turnover Rate (Risk Percentage)')\n",
    "axes[1].set_xlabel('Salary Level')\n",
    "axes[1].set_ylabel('Turnover Rate (%)')\n",
    "\n",
    "# Add average line for context\n",
    "avg_turnover = (data['Left'].mean() * 100)\n",
    "axes[1].axhline(avg_turnover, color='red', linestyle='--', label=f'Avg: {avg_turnover:.1f}%')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the exact numbers to confirm\n",
    "print(\"Turnover Rate by Salary:\")\n",
    "print(salary_counts['Turnover_Rate'].to_markdown(floatfmt=\".1f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c45a1",
   "metadata": {},
   "source": [
    "**Volume vs Rate**  \n",
    " The chart on the left displays a count for each salary level which may sometimes lead to misintepretation by viewers. In order to avoid this, the chart on the right displays the turnover rate for each department. Observing the chart we can see that despite the low and medium salary levels have similar count, we can see that low salary employees face a higher turnover rate, likely meaning that employees could benefit from higher compensation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d157fe",
   "metadata": {},
   "source": [
    "### 3.4 Turnover by Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Turnover Rate by Tenure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate rates\n",
    "tenure_metrics = data.groupby('TimeSpentAtCompany')['Left'].mean().reset_index()\n",
    "tenure_metrics['Left'] = tenure_metrics['Left'] * 100 # Convert to percentage\n",
    "\n",
    "sns.barplot(x='TimeSpentAtCompany', y='Left', data=tenure_metrics)\n",
    "\n",
    "plt.title('Turnover Rate by Years')\n",
    "plt.xlabel('Years at Company')\n",
    "plt.ylabel('Turnover Rate (%)')\n",
    "plt.axhline(data['Left'].mean()*100, color='red', linestyle='--', label='Company Avg')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498a5d2",
   "metadata": {},
   "source": [
    "**Year 3 Retention Program**  \n",
    "Due to the increase in employee turnover rate in years 4-5, HR should intervine during the 3rd year of employment with either:\n",
    "1. promotions\n",
    "2. raises\n",
    "3. rotation programs  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2cdde",
   "metadata": {},
   "source": [
    "### 3.5 Coorelation Between Workload and Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a38d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Turnover vs. Number of Projects\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# We use a pointplot here to show the \"U-Shaped\" curve clearly\n",
    "sns.pointplot(x='NumberofProjects', y='Left', data=data, color='purple', errorbar=None)\n",
    "\n",
    "plt.title('Turnover Rate by Number of Projects')\n",
    "plt.xlabel('Number of Projects')\n",
    "plt.ylabel('Probability of Leaving')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e891d0d",
   "metadata": {},
   "source": [
    "**Sweet Spot for Project**  \n",
    " Employees handling between 3-5 projects have a significanly lower turnover rates. This is because any lower than 3 may lead to boredom in an employee, and handling a excessivly high number of projects, over 6, can lead to potential burnout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82619f",
   "metadata": {},
   "source": [
    "### 3.6 Overall Workload Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Projects vs. Monthly Hours (Aggregated)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Removing the 'hue' argument aggregates the data for each project level\n",
    "sns.boxplot(\n",
    "    x='NumberofProjects', \n",
    "    y='AverageMonthlyHours', \n",
    "    data=data, \n",
    "    color='tab:blue'\n",
    ")\n",
    "plt.title('Aggregated Monthly Hours Distribution by Number of Projects (Company Benchmark)')\n",
    "plt.xlabel('Number of Projects')\n",
    "plt.ylabel('Average Monthly Hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17511398",
   "metadata": {},
   "source": [
    "**Aggregated Workload Insight**\n",
    "This chart establishes the baseline for how workload is distributed across the organization. The key takeaways are:\n",
    " \n",
    "* **Linear Relationship:** There is a clear, nearly linear relationship between the number of projects and the median hours worked.\n",
    "* **Peak Workload:** The highest median hours worked is for employees with **7 projects**.\n",
    "* **Context:** This chart is essential context for the attrition analysis, as it shows what is considered 'normal' workload for the organization before diving into the specific attrition risk factors.\n",
    " \n",
    "**Synthesis:** This chart confirms that the project volume identified in the previous **Project Load vs. Tunover Rate** analysis corresponds to a moderate workload for the \"Goldilocks Zone.\" **Crucially, managing project assignments to remain within the 3-5 project zone directly enables management to contain Average Monthly Hours within a moderate, less stressful range, therby lowering burnout risk.** The high median hours at 6 and 7 projects sets the stage for the final analysis, where this excessive workload defines the high-risk 'Burned Out\" cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc218265",
   "metadata": {},
   "source": [
    "### 3.7 Satisfaction Level Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 Satisfaction Level Distribution (KDE Plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.kdeplot(data=data[data['Left'] == 1]['SatisfactionLevel'], label='Left', fill=True, color='red', alpha=0.3)\n",
    "sns.kdeplot(data=data[data['Left'] == 0]['SatisfactionLevel'], label='Stayed', fill=True, color='blue', alpha=0.3)\n",
    "\n",
    "plt.title('Distribution of Satisfaction Level: Stayed vs. Left')\n",
    "plt.xlabel('Satisfaction Level')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a366d64",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "The Kernel Density Estimation (KDE) plot reveals a critical insight: turnover is not linear. If turnover were driven solely by unhappiness, we would see a single spike on the left side of the chart. Instead, the \"Left\" distribution (Red curve) is **tri-modal**, displaying three distinct groups of employees leaving the company:\n",
    " \n",
    "1. **Extremely Dissatisfied (Peak at ~0.1):** A large group of employees are leaving with near-zero satisfaction scores, indicating a severe disconnect with the company.\n",
    "2. **Moderately Dissatisfied (Peak at ~0.4):** A second distinct group exists in the lower-middle range, suggesting a different set of friction points than the first group.\n",
    "3. **The \"Happy Leaver\" Paradox (Peak at ~0.8):** Most concerningly, there is a significant spike of employees leaving despite having **high satisfaction levels**. This contradicts the assumption that \"happy employees stay\" and suggests that factors outside of satisfaction are driving this specific group away.\n",
    " \n",
    "**Next Step:**\n",
    "Since satisfaction levels alone cannot explain *why* these three distinct groups are formed, we must next layer in **Workload (Hours)** and **Performance (Evaluations)** to profile these groups accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d901a",
   "metadata": {},
   "source": [
    "### 3.8 Satisfaction vs Evaluation/Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.7 Side-by-Side EDA Comparison\n",
    "\n",
    "# 1. Create a temporary column with readable labels\n",
    "data['Status'] = data['Left'].map({0: 'Stayed', 1: 'Left'})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Define colors: Stayed = Blue, Left = Red\n",
    "custom_palette = {'Stayed': 'blue', 'Left': 'red'}\n",
    "\n",
    "# --- Chart 1: Satisfaction vs. Evaluation (Left Side) ---\n",
    "sns.scatterplot(\n",
    "    x='SatisfactionLevel', \n",
    "    y='LastEvaluation', \n",
    "    hue='Status', \n",
    "    data=data, \n",
    "    palette=custom_palette, \n",
    "    alpha=0.4, \n",
    "    s=40,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Satisfaction vs. Last Evaluation')\n",
    "axes[0].set_xlabel('Satisfaction Level')\n",
    "axes[0].set_ylabel('Last Evaluation Score')\n",
    "# Force legend to bottom left\n",
    "axes[0].legend(loc='lower left', title='Status', frameon=True)\n",
    "\n",
    "\n",
    "# --- Chart 2: Satisfaction vs. Monthly Hours (Right Side) ---\n",
    "sns.scatterplot(\n",
    "    x='SatisfactionLevel', \n",
    "    y='AverageMonthlyHours', \n",
    "    hue='Status', \n",
    "    data=data, \n",
    "    palette=custom_palette, \n",
    "    alpha=0.4, \n",
    "    s=40,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Satisfaction vs. Average Monthly Hours')\n",
    "axes[1].set_xlabel('Satisfaction Level') \n",
    "axes[1].set_ylabel('Average Monthly Hours')\n",
    "axes[1].legend(loc='lower left', title='Status', frameon=True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up: Drop the helper column so it doesn't duplicate data later\n",
    "data.drop('Status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138da77",
   "metadata": {},
   "source": [
    "**Coorelation between AVG monthly hours and Evaluation Score**  \n",
    "Despite the charts above representing different factors on the y-axis, we can see that the values and clusters remain in the same general area. This shows a direct coorelation between the hours an employee puts in and their evaluation score. We also see 3 distinct clusters between the employees who left.\n",
    "1. **Burned Out** (top left)\n",
    "    * **Profile:** Satisfaction < 0.2 | Evaluation > 0.75 | Hours > 250\n",
    "    * **Diagnosis:** These are high-performing employees who were severely overworked. Their \"crash\" in satisfaction (to ~0.1) is directly correlated with working significantly more hours than the company average.\n",
    "2. **Underperformers** (lower)\n",
    "    * **Profile:** Satisfaction ~ 0.4 | Evaluation < 0.55 | Hours < 160\n",
    "    * **Diagnosis:** This group corresponds to the \"Underperformer\" peak. They worked the fewest hours and received low evaluation scores, indicating a lack of engagement or \"Quiet Quitting.\"\n",
    "3. **Superstars** (top right)\n",
    "    * **Profile:** Satisfaction > 0.7 | Evaluation > 0.8 | Hours > 220\n",
    "    * **Diagnosis:** This explains the \"Happy Leaver\" paradox. These employees were highly productive and put in long hours, yet remained satisfied. Their departure suggests they were likely poached by competitors offering better compensation or growth, rather than fleeing a bad work environment.\n",
    " \n",
    "Our analysis reveals that turnover is not random; it is driven by three specific structural failures: Overworking our best talent (Burnout), failing to manage poor performers (Mismatched), and failing to retain happy high-achievers (likely Compensation/Growth issues).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5339e3c",
   "metadata": {},
   "source": [
    "## Step 4: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Data Preprocessing  ---\n",
    "print(\"--- Step 4: Data Preprocessing ---\")\n",
    "\n",
    "# Define our target variable (y) and features (X)\n",
    "target = 'Left'\n",
    "\n",
    "# We only drop the target itself ('Left')\n",
    "features = data.drop([target], axis=1) \n",
    "y = data[target]\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numeric_features = [\n",
    "    'SatisfactionLevel', \n",
    "    'LastEvaluation', \n",
    "    'NumberofProjects', \n",
    "    'AverageMonthlyHours', \n",
    "    'TimeSpentAtCompany', \n",
    "    'PromotionsInLast5Years',\n",
    "    'WorkAccident'\n",
    "]\n",
    "\n",
    "categorical_features = ['Department', 'Salary']\n",
    "\n",
    "print(f\"Target variable: {target}\")\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\\n\")\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "# 1. For numeric features: Scale them\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# 2. For categorical features: One-Hot Encode them\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Use ColumnTransformer to apply transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created.\")\n",
    "print(\"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23291a4a",
   "metadata": {},
   "source": [
    "## Step 5: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6956905e",
   "metadata": {},
   "source": [
    "### 1. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set:  {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f216f68",
   "metadata": {},
   "source": [
    "### 2. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd72773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.2: Model Implementation ---\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel='linear', random_state=42),\n",
    "    \"MLP (Neural Net)\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=600, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Pipeline: Preprocess -> Train\n",
    "    clf_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    clf_pipeline.fit(X_train, y_train)\n",
    "    y_pred = clf_pipeline.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "print(\"\\nTraining Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcccd72",
   "metadata": {},
   "source": [
    "## Step 6: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa77198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Model Comparison ---\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=results_df['F1-Score'], y=results_df.index)\n",
    "plt.title('Model Precision Comparison')\n",
    "plt.xlim(0, 1.0)\n",
    "plt.show()\n",
    "\n",
    "# Display Table\n",
    "print(\"Detailed Performance Metrics:\")\n",
    "display(results_df) #prints table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207436b",
   "metadata": {},
   "source": [
    "**Accuracy:** How often is the model correct?  \n",
    "**Precision:** When the model predicts someone will leave, how often is it correct?  \n",
    "**Recall:** Out of the people who quit, what percentage did the model catch beforehand?  \n",
    "**F1-Score:** How reliable is the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323fe427",
   "metadata": {},
   "source": [
    "### Visualization of Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8077a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization: Confusion Matrix (Random Forest) ---\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(\"--- Visualizing Confusion Matrix for Random Forest ---\")\n",
    "\n",
    "# build and train the Random Forest pipeline \n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 1. Generate predictions using the fitted Random Forest pipeline from Step 6\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "# 2. Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# 3. Plot the matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "# display_labels maps 0 to 'Stayed' and 1 to 'Left' for clarity\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Stayed', 'Left'])\n",
    "disp.plot(cmap='Blues', values_format='d') # 'd' formats numbers as integers (no scientific notation)\n",
    "\n",
    "plt.title('Confusion Matrix: Random Forest')\n",
    "plt.grid(False) # Turn off grid lines for cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beacfd3",
   "metadata": {},
   "source": [
    "Recall would be shown on the bottom row, while precision is shown on the right column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f27dd",
   "metadata": {},
   "source": [
    "## Step 7: Feature Importance Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Feature Importance Analysis ---\n",
    "print(\"--- Step 7: Feature Importance Analysis (from Random Forest) ---\")\n",
    "\n",
    "# --- Extract Feature Names ---\n",
    "# 1. Get the categorical names after OneHotEncoding\n",
    "cat_feature_names = rf_pipeline.named_steps['preprocessor'] \\\n",
    "                               .named_transformers_['cat'] \\\n",
    "                               .get_feature_names_out(categorical_features)\n",
    "\n",
    "# 2. Combine with the numeric features\n",
    "all_feature_names = numeric_features + list(cat_feature_names)\n",
    "\n",
    "# --- Extract Importances ---\n",
    "importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# Create a DataFrame for easy viewing\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the Top 10\n",
    "print(\"Top 10 factors influencing employee retention:\")\n",
    "print(feature_importance_df.head(10).to_markdown(index=False, floatfmt=\".4f\"))\n",
    "\n",
    "# --- Visualization: Feature Importance ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))\n",
    "plt.title('Top 10 Features Influencing Employee Retention')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18276a03",
   "metadata": {},
   "source": [
    "## Step 8: Prediction Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 8: Employee Risk Scoring ---\n",
    "print(\"--- Step 8: Employee Risk Scoring ---\")\n",
    "\n",
    "# 1. Get probability scores\n",
    "risk_scores = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Create a DataFrame to analyze these scores\n",
    "risk_df = X_test.copy()\n",
    "risk_df['Risk_Score'] = risk_scores\n",
    "risk_df['Actual_Status'] = y_test \n",
    "\n",
    "# --- Filter for ONLY Current Employees ---\n",
    "# We only want to look at people who are currently at the company (Actual_Status == 0)\n",
    "current_employee_risk = risk_df[risk_df['Actual_Status'] == 0].copy()\n",
    "\n",
    "# 3. Segment employees into groups (Low, Medium, High Risk)\n",
    "def categorize_risk(score):\n",
    "    if score < 0.3:\n",
    "        return 'Low Risk'\n",
    "    elif score < 0.7:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'High Risk'\n",
    "\n",
    "current_employee_risk['Risk_Group'] = current_employee_risk['Risk_Score'].apply(categorize_risk)\n",
    "\n",
    "# 4. Display ALL \"High Risk\" Current Employees\n",
    "print(\"High Risk Employees:\")\n",
    "print(current_employee_risk[current_employee_risk['Risk_Group'] == 'High Risk'].sort_values(by='Risk_Score', ascending=False).to_markdown())\n",
    "\n",
    "# 5. Analyze the High Risk Group\n",
    "high_risk_employees = current_employee_risk[current_employee_risk['Risk_Group'] == 'High Risk']\n",
    "print(f\"\\nNumber of CURRENT employees identified as High Risk: {len(high_risk_employees)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9: Global Cluster Analysis (The \"Ghost Map\") ---\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. Select Features for Clustering\n",
    "# We use these three because they visually define the \"Leaver\" profiles best\n",
    "cluster_features = ['SatisfactionLevel', 'LastEvaluation', 'AverageMonthlyHours']\n",
    "X_clustering = data[cluster_features].copy()\n",
    "\n",
    "# 2. Scale the data (Crucial for K-Means)\n",
    "cluster_scaler = StandardScaler()\n",
    "X_scaled = cluster_scaler.fit_transform(X_clustering)\n",
    "\n",
    "# 3. Fit K-Means & Create the Column\n",
    "# This fixes your error! We are creating the 'Archetype_Cluster' column here.\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "data['Archetype_Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# --- Step 8: Risk Scoring & Action Plan Chart ---\n",
    "print(\"--- Step 9: Risk Scoring Current Employees ---\")\n",
    "\n",
    "# 1. Isolate Current Employees\n",
    "current_employee_mask = data['Left'] == 0\n",
    "current_employees = data[current_employee_mask].copy()\n",
    "\n",
    "# 2. Prepare Data for Prediction (Safety Step)\n",
    "# We filter to only the columns the model was trained on to prevent errors\n",
    "training_features = list(rf_pipeline.named_steps['preprocessor'].feature_names_in_)\n",
    "X_current_clean = current_employees[training_features]\n",
    "\n",
    "# 3. Generate Probabilities\n",
    "risk_probs = rf_pipeline.predict_proba(X_current_clean)[:, 1]\n",
    "\n",
    "# 4. Assign Scores back to the Dataframe\n",
    "data.loc[current_employee_mask, 'Risk_Score'] = risk_probs\n",
    "data['Risk_Score'] = data['Risk_Score'].fillna(0)\n",
    "\n",
    "# 5. Identify \"High Risk\" Current Employees (> 70%)\n",
    "high_risk_mask = (data['Left'] == 0) & (data['Risk_Score'] > 0.7)\n",
    "print(f\"High Risk Current Employees identified: {high_risk_mask.sum()}\")\n",
    "\n",
    "# 6. Chart 2: The Action Plan (Red Dots)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Layer 1: Background Context (Faint)\n",
    "sns.scatterplot(\n",
    "    x='SatisfactionLevel', \n",
    "    y='LastEvaluation', \n",
    "    hue='Archetype_Cluster', \n",
    "    data=data, \n",
    "    palette='viridis', \n",
    "    s=50, \n",
    "    alpha=0.2,            # Faded background\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Layer 2: High Risk Current Employees (Bright Red)\n",
    "sns.scatterplot(\n",
    "    x='SatisfactionLevel',\n",
    "    y='LastEvaluation',\n",
    "    data=data[high_risk_mask],\n",
    "    color='red',\n",
    "    marker='o',\n",
    "    s=150,                # Big dots\n",
    "    edgecolor='black',\n",
    "    label='High Risk Current Employee (>70%)'\n",
    ")\n",
    "\n",
    "plt.title('Action Plan: High-Risk Employees Mapped to Turnover Archetypes')\n",
    "plt.xlabel('Satisfaction Level')\n",
    "plt.ylabel('Last Evaluation')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Display the High Risk list\n",
    "print(\"Actionable List (High Risk):\")\n",
    "cols_to_show = ['Department', 'SatisfactionLevel', 'LastEvaluation', 'Risk_Score']\n",
    "print(data[high_risk_mask].sort_values('Risk_Score', ascending=False)[cols_to_show].head(5).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0bd70",
   "metadata": {},
   "source": [
    "## Final Executive Recommendation: Targeted Intervention Plan\n",
    "\n",
    "**The Situation:**\n",
    "Applying our Predictive Model (Random Forest) to the current employee roster, we have identified **3 specific employees** who currently hold a **>70% probability of leaving** the company.\n",
    "\n",
    "1. **The Superstars** (2 Employees)\n",
    "* **Location:** Top-Right Purple Cluster (High Satisfaction, High Evaluation).\n",
    "* **Diagnosis:** These distinct red dots represent our top talent. They are performing well and are generally satisfied, yet the model flags them as high risk. This indicates they are likely being **poached** by competitors offering higher salaries or better titles.\n",
    "* **Action Plan:** **Immediate Retention.**    * Do not offer \"culture\" fixes (they are already happy). \n",
    "  * HR should immediately review their compensation packages and career paths. \n",
    "  * **Recommendation:** Conduct a \"Stay Interview\" this week and be prepared to counter-offer.\n",
    "\n",
    "2. **The Mismatched** (1 Employee)\n",
    "* **Location:** Bottom-Middle Teal Cluster (Low Satisfaction, Low Evaluation).\n",
    "* **Diagnosis:** This employee is disengaged, working low hours, and performing poorly. They fit the \"Quiet Quitting\" profile.\n",
    "* **Action Plan:** **Performance Management.**\n",
    "  * Spending retention budget here would be a waste of resources.\n",
    "  * **Recommendation:** Place this employee on a Performance Improvement Plan (PIP). If performance does not improve, allow natural attrition to occur.\n",
    "\n",
    "**Summary:**\n",
    "We have moved from reactive analysis to proactive prevention. By focusing our efforts on the **two \"Superstars\"** identified above, we can protect the company's most valuable assets while avoiding unnecessary spending on the \"Mismatched\" case.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
